{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 2\n",
    "\n",
    "Implement basic backward pass in MLP. Perform forward and backward propagation through your network and check your gradients.\n",
    "This time, the forward pass is implemented for you. Notice the matrix notation - the dimensions are in form $[m,nX,1]$, where $m$ is batch size (number of samples) and $nX$ is the size of sample vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "Implement derivations of standard activation functions (ReLU, Sigmoid), which are used in your task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   ActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class ActivationFunction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, z):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   LinearActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class LinearActivationFunction(ActivationFunction):\n",
    "    def __call__(self, z):\n",
    "        return z\n",
    "\n",
    "    def derivation(self, z):\n",
    "        return 1\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class RELUActivationFunction(ActivationFunction):\n",
    "    def __call__(self, z):\n",
    "        return np.maximum(z, 0)\n",
    "\n",
    "    def derivation(self, z):\n",
    "        return int(x>=0)\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class SigmoidActivationFunction(ActivationFunction):\n",
    "    def __call__(self, z):\n",
    "        return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "    def derivation(self, z):\n",
    "        a = self(z)\n",
    "        return np.multiply(self(z),(1-self(z)))\n",
    "    \n",
    "# Activation mapping\n",
    "    \n",
    "MAP_ACTIVATION_FUCTIONS = {\n",
    "    \"linear\": LinearActivationFunction,\n",
    "    \"relu\": RELUActivationFunction,\n",
    "    \"sigmoid\": SigmoidActivationFunction\n",
    "}\n",
    "\n",
    "def CreateActivationFunction(kind):\n",
    "    if (kind in MAP_ACTIVATION_FUCTIONS):\n",
    "        return MAP_ACTIVATION_FUCTIONS[kind]()\n",
    "    raise ValueError(kind, \"Unknown activation function {0}\".format(kind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer\n",
    "\n",
    "This is the main class which can hold different types of layers and provides us with standard tasks like forward propagation. Implement backward functions for defined classes.\n",
    "\n",
    "nUnits - number of neuron units in your layer\n",
    "\n",
    "prevLayer - previous layer (need it to know the shape of it to create appropriate number of weights for you to use in current layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Layer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Layer:\n",
    "    def __init__(self, act=\"linear\", name=\"layer\"):\n",
    "        self.shape = (0, 0)\n",
    "        self.activation = CreateActivationFunction(act)\n",
    "        self.name = name\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   InputLayer class\n",
    "#------------------------------------------------------------------------------\n",
    "class InputLayer(Layer):\n",
    "    def __init__(self, nUnits, name=\"Input\"):\n",
    "        super().__init__(act=\"linear\", name=name)\n",
    "        self.nUnits = nUnits\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        self.shape = (self.nUnits, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "\n",
    "    def backward(self, X):\n",
    "        return None\n",
    "    \n",
    "#------------------------------------------------------------------------------\n",
    "#   Basic Dense Layer class\n",
    "#------------------------------------------------------------------------------\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, nUnits, act=\"linear\", name=\"Dense\"):\n",
    "        super().__init__(act, name=name)\n",
    "        # init each neuron into list        \n",
    "        self.nUnits = nUnits\n",
    "        self.W = None\n",
    "        self.b = None\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        #initialize all neurons\n",
    "        self.shape = (self.nUnits, prevLayer.shape[0])\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        prev_nUnits, _ = prevLayer.shape\n",
    "        self.W = np.random.randn(self.nUnits, prev_nUnits)\n",
    "        self.b = np.zeros((self.nUnits, 1), dtype=float)\n",
    "\n",
    "    def forward(self, X):\n",
    "        print(\"Forward of\", self.name)\n",
    "        self.z = np.matmul(self.W, X) + self.b         # Z = W*x + b\n",
    "        self.a = self.activation(self.z)               # a = activation(Z)\n",
    "        \n",
    "        return self.a\n",
    "\n",
    "    def backward(self, da, aPrev):\n",
    "        #   da  =   dLoss -> dL/da of previous layer - with respect to backward pass\n",
    "        #   aPrev   =   activation of previous layer needed for weights - with respect to forward pass\n",
    "        batch_size = aPrev.shape[0]\n",
    "        print(\"Backward of\", self.name)\n",
    "        _,m = da.shape\n",
    "        \n",
    "        dz = np.multiply(da,self.activation.derivation(z))\n",
    "        dW = (1./m)*np.matmul(dz, aPrev.T)\n",
    "        db = (1./m)*np.sum(dz,axis=1, keepdims=True)\n",
    "        \n",
    "        daPrev = np.matmul(W.T,dz)\n",
    "        \n",
    "        return daPrev\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "\n",
    "Implement two standard loss functions (Binary Cross Entropy and Mean Squared Error), which you will/can use in your implementation of MLP backward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   LossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class LossFunction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, A, Y):\n",
    "        pass\n",
    "\n",
    "    def derivation(self, A, Y):\n",
    "        pass\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   BinaryCrossEntropyLossFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class BinaryCrossEntropyLossFunction(LossFunction):\n",
    "    def __call__(self, A, Y):\n",
    "        # Warning! Use of logarithm - Take care about definition scope\n",
    "        return -(np.multiply(Y,np.log(A))+np.multiply((1-Y),np.log(1-A)))\n",
    "    \n",
    "    def derivation(self, A, Y):\n",
    "        # Warning! Use of logarithm - Take care about definition scope\n",
    "        return (-np.divide(Y,A)+np.divide((1-Y),(1-A)))\n",
    "        \n",
    "    \n",
    "class MeanSquaredErrorLossFunction(LossFunction):\n",
    "    def __call__(self, A, Y): # loss = (A-Y)^2\n",
    "        return np.square(np.substract(A,Y))\n",
    "\n",
    "    def derivation(self, A, Y): #dLoss = -1*(2(A-Y))\n",
    "        return np.multpliy(-2,np.subtract(Y,A))\n",
    "\n",
    "\n",
    "MAP_LOSS_FUNCTIONS = {\n",
    "    \"bce\": BinaryCrossEntropyLossFunction,\n",
    "    \"mse\": MeanSquaredErrorLossFunction\n",
    "}\n",
    "\n",
    "def CreateLossFunction(kind):\n",
    "    if (kind in MAP_LOSS_FUNCTIONS):\n",
    "        return MAP_LOSS_FUNCTIONS[kind]()\n",
    "    raise ValueError(kind, \"Unknown loss function {0}\".format(kind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class\n",
    "\n",
    "This is the basic class which holds all of your layers and encapsulate functionality to predict results from your input as a forward pass through all the layers after you create your model and initialize all the layers.\n",
    "\n",
    "Implemet backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model:\n",
    "    def __init__(self, lossName):\n",
    "        self.layers = []\n",
    "        # Initialize loss function\n",
    "        self.loss_fn = CreateLossFunction(lossName)\n",
    "        \n",
    "    def addLayer(self,  layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def initialize(self):\n",
    "        # Call initialization sequentially on all layers\n",
    "        prevLayer = None\n",
    "        for l in self.layers:\n",
    "            l.initialize(prevLayer)\n",
    "            prevLayer = l      \n",
    "    \n",
    "    def forward(self, X):\n",
    "        # Single feed forward\n",
    "        A = X\n",
    "        for l in self.layers:\n",
    "            A = l.forward(A)\n",
    "            \n",
    "        return A  \n",
    "    \n",
    "    def backward(self, dLoss):\n",
    "        da = dLoss\n",
    "        for layer, lPrev in zip(self.layers[::-1], self.layers[-2::-1]):\n",
    "            da = layer.backward(da, lPrev.a)\n",
    "            \n",
    "        \n",
    "    \n",
    "    def compute_loss(self, A, Y):\n",
    "        batch_size = Y.shape[0]\n",
    "        \n",
    "        return self.loss_fn(A,Y)\n",
    "        \n",
    "    \n",
    "    def derive_loss(self, A, Y):\n",
    "        batch_size = Y.shape[0]\n",
    "        \n",
    "        return self.loss_fn.derivation(self.compute_loss(A,Y),Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Cell\n",
    "\n",
    " 1. Initialize dataset. \n",
    " 2. Declare a simple model (at least 4 layer) with relu on hidden layers and sigmoid on output layer.\n",
    " 3. Perform forward pass through the network. \n",
    " 4. Compute loss.\n",
    " 5. Derive loss.\n",
    " 6. Perform backward pass.\n",
    " 7. Celebrate and scroll lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward of 1st_Layer\n",
      "Forward of 2nd_Layer\n",
      "Forward of 3rd_Layer\n",
      "Forward of 4th_Layer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(16, 1, 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Main processing\n",
    "from dataset import dataset_Circles\n",
    "# Task A:\n",
    "\n",
    "X, Y = dataset_Circles(n=16, radius=0.7, noise=0.0)\n",
    "\n",
    "\n",
    "model = Model(lossName = \"bce\")\n",
    "model.addLayer( InputLayer(nUnits=2, name=\"input_layer\"))\n",
    "model.addLayer( DenseLayer(nUnits=6, act=\"relu\", name=\"1st_Layer\"))\n",
    "model.addLayer( DenseLayer(nUnits=3, act=\"relu\", name=\"2nd_Layer\"))\n",
    "model.addLayer( DenseLayer(nUnits=2, act=\"relu\", name=\"3rd_Layer\"))\n",
    "model.addLayer( DenseLayer(nUnits=1, act=\"sigmoid\", name=\"4th_Layer\"))\n",
    "\n",
    "model.initialize()\n",
    "\n",
    "A  = model.forward(X)\n",
    "\n",
    "loss = model.derive_loss(A,Y)\n",
    "loss.shape\n",
    "# model.backward(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**How does gradient checking work?**.\n",
    "\n",
    "As in 1) and 2), you want to compare \"gradapprox\" to the gradient computed by backpropagation. The formula is still:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial \\theta} = \\lim_{\\varepsilon \\to 0} \\frac{J(\\theta + \\varepsilon) - J(\\theta - \\varepsilon)}{2 \\varepsilon} \\tag{1}$$\n",
    "\n",
    "However, $\\theta$ is not a scalar anymore. It is a dictionary called \"parameters\". We implemented a function \"`dictionary_to_vector()`\" for you. It converts the \"parameters\" dictionary into a vector called \"values\", obtained by reshaping all parameters (W1, b1, W2, b2, W3, b3) into vectors and concatenating them.\n",
    "\n",
    "The inverse function is \"`vector_to_dictionary`\" which outputs back the \"parameters\" dictionary.\n",
    "\n",
    "\n",
    "We have also converted the \"gradients\" dictionary into a vector \"grad\" using gradients_to_vector(). You don't need to worry about that.\n",
    "\n",
    "\n",
    "Here is pseudo-code that will help you implement the gradient check.\n",
    "\n",
    "For each i in num_parameters:\n",
    "- To compute `J_plus[i]`:\n",
    "    1. Set $\\theta^{+}$ to `np.copy(parameters_values)`\n",
    "    2. Set $\\theta^{+}_i$ to $\\theta^{+}_i + \\varepsilon$\n",
    "    3. Calculate $J^{+}_i$ using to `forward_propagation_n(x, y, vector_to_dictionary(`$\\theta^{+}$ `))`.     \n",
    "- To compute `J_minus[i]`: do the same thing with $\\theta^{-}$\n",
    "- Compute $gradapprox[i] = \\frac{J^{+}_i - J^{-}_i}{2 \\varepsilon}$\n",
    "\n",
    "Thus, you get a vector gradapprox, where gradapprox[i] is an approximation of the gradient with respect to `parameter_values[i]`. You can now compare this gradapprox vector to the gradients vector from backpropagation. Just like for the 1D case (Steps 1', 2', 3'), compute: \n",
    "$$ difference = \\frac {\\| grad - gradapprox \\|_2}{\\| grad \\|_2 + \\| gradapprox \\|_2 } \\tag{3}$$\n",
    "\n",
    "\n",
    "**The code will be added later** but soon enough ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: gradient_check_n\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verification cell\n",
    "\n",
    " 8. Verify your solution by gradient checking.\n",
    " 9. Start crying.\n",
    " 10. Repeat until correct ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_check_n(network, X, Y, epsilon = 1e-7):\n",
    "    \"\"\"\n",
    "    Checks if backward_propagation_n computes correctly the gradient of the cost output by forward_propagation_n\n",
    "\n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", \"W2\", \"b2\", \"W3\", \"b3\":\n",
    "    grad -- output of backward_propagation_n, contains gradients of the cost with respect to the parameters.\n",
    "    x -- input datapoint, of shape (input size, 1)\n",
    "    y -- true \"label\"\n",
    "    epsilon -- tiny shift to the input to compute approximated gradient with formula(1)\n",
    "\n",
    "    Returns:\n",
    "    difference -- difference (2) between the approximated gradient and the backward propagation gradient\n",
    "    \"\"\"\n",
    "\n",
    "    # Set-up variables\n",
    "    gradapprox = []\n",
    "    grad_backward = []\n",
    "\n",
    "    for i,layer in enumerate(network.layers):\n",
    "        # Compute gradapprox\n",
    "        if i < 1:\n",
    "            continue\n",
    "        shape = layer.W.shape\n",
    "        # print(shape[0], ',', shape[1])\n",
    "        for i in range(shape[0]):\n",
    "            for j in range(shape[1]):\n",
    "                # print('i',i,'j',j)\n",
    "                # Compute J_plus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_plus[i]\".\n",
    "                # \"_\" is used because the function you have to outputs two parameters but we only care about the first one\n",
    "                origin_W = layer.W[i][j]\n",
    "\n",
    "                layer.W[i][j] = origin_W + epsilon\n",
    "                A_plus = network.forward(X)\n",
    "                J_plus = network.compute_loss(A_plus, Y)\n",
    "\n",
    "                # Compute J_minus[i]. Inputs: \"parameters_values, epsilon\". Output = \"J_minus[i]\".\n",
    "                layer.W[i][j] = origin_W - epsilon\n",
    "                A_minus = network.forward(X)\n",
    "                J_minus = network.compute_loss(A_minus, Y)\n",
    "\n",
    "                # Compute gradapprox[i]\n",
    "                gradapprox.append((J_plus - J_minus) / (2*epsilon))\n",
    "                # print(layer.name, layer.dW.shape)\n",
    "                # grad = np.mean(layer.dW, axis=0, keepdims=True)\n",
    "                # grad_backward.append(grad[0][i][j])\n",
    "                grad_backward.append(layer.dW[i][j])\n",
    "\n",
    "\n",
    "    # Compare gradapprox to backward propagation gradients by computing difference.\n",
    "\n",
    "    gradapprox = np.reshape(gradapprox, (-1, 1))\n",
    "    grad_backward = np.reshape(grad_backward, (-1, 1))\n",
    "\n",
    "    numerator = np.linalg.norm(grad_backward - gradapprox)\n",
    "    denominator = np.linalg.norm(grad_backward) + np.linalg.norm(gradapprox)\n",
    "    difference = numerator / denominator\n",
    "\n",
    "    if difference > 2e-7:\n",
    "        print (\"\\033[91m\" + \"There is a mistake in the backward propagation! difference = \" + str(difference) + \"\\033[0m\")\n",
    "    else:\n",
    "        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(difference) + \"\\033[0m\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
