{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Implement basic layer which will consist of simple logical neurons. Allow your solution to stack multiple layers to form MLP network. Perform forward propagation through your network.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Processing Cell\n",
    "\n",
    "Perform following operations:\n",
    "\n",
    "__A__: Declare a simple model consisting of Input Layer, followed by 1 Dense Layer with single neuron. Perform forward pass for the data example __xIn__ of batch size = 10 and feature vector size = 3. Plot the results. Repeat the process using all 3 activation functions.\n",
    "\n",
    "__B__: Declare a simple model consisting of Input Layer, followed by 3 Dense Layers of arbitrary size and 1 Dense Layer with 2 neurons. Declare your own input data with batch size 16 and vector size of 10. Perform forward pass through the network and visualize activations of each resulting neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xIn = np.array([[-4, -4, -4],\n",
    "               [-3, -3, -3],\n",
    "               [-2, -2, -2],\n",
    "               [-1, -1, -1],\n",
    "               [-0, -0, -0],\n",
    "               [1, 1, 1],\n",
    "               [2, 2, 2],\n",
    "               [3, 3, 3],\n",
    "               [4, 4, 4],\n",
    "               [5, 5, 5]])\n",
    "###>>> start of solution\n",
    "m = Model()\n",
    "m.addLayer(InputLayer(3)) # number of input features per data\n",
    "m.addLayer(DenseLayer(1, act=\"linear\"))\n",
    "m.initialize()\n",
    "\n",
    "y = m.predict(xIn)\n",
    "###<<< end of solution\n",
    "plt.plot(y)\n",
    "plt.show()\n",
    "\n",
    "###>>> start of solution\n",
    "m = Model()\n",
    "m.addLayer(InputLayer(3))\n",
    "m.addLayer(DenseLayer(1, act=\"sigmoid\"))\n",
    "m.initialize()\n",
    "\n",
    "y = m.predict(xIn)\n",
    "###<<< end of solution\n",
    "plt.plot(y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###>>> start of solution\n",
    "m = Model()\n",
    "m.addLayer(InputLayer(3))\n",
    "m.addLayer(DenseLayer(1, act=\"relu\"))\n",
    "m.initialize()\n",
    "\n",
    "y = m.predict(xIn)\n",
    "###<<< end of solution\n",
    "plt.plot(y)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n",
    "# Task B:\n",
    "xIn = np.random.randn(16, 10)\n",
    "###>>> start of solution\n",
    "m = Model()\n",
    "m.addLayer(InputLayer(10))\n",
    "m.addLayer(DenseLayer(10, act=\"sigmoid\"))\n",
    "m.addLayer(DenseLayer(5, act=\"sigmoid\"))\n",
    "m.addLayer(DenseLayer(2, act=\"sigmoid\"))\n",
    "m.initialize()\n",
    "\n",
    "y = m.predict(xIn)\n",
    "###<<< end of solution\n",
    "y = np.transpose(y) # predictions are per data for each neuron, we want to plot all activations from first and second neuron to compare\n",
    "plt.plot(y[0])\n",
    "plt.plot(y[1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layer\n",
    "\n",
    "This is the main class which can hold different types of layers and provide us with standard tasks like forward propagation. Perform the variable declaration, provide your solution with variable initialization when later called by your model and also functionality to perform forward pass. We added another class of Neuron to the Dense Layer for simplicity. You can perform the logic of a layer per single neuron using lists and for cycles. You can declare activation function in your __init__ function of the parent class (Layer) and call it later when you perform forward pass, or perform it in one of the child classes.\n",
    "\n",
    "nUnits - number of neuron units in your layer\n",
    "\n",
    "prevLayer - previous layer (need it to know the shape of it to create appropriate number of weights for you to use in current layer)\n",
    "\n",
    "nX - number of units (in case of input layer number of features for each data vector)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Layer class\n",
    "#------------------------------------------------------------------------------\n",
    "class Layer:\n",
    "    def __init__(self, act=\"linear\"):\n",
    "        self.shape = (0, 0)\n",
    "        #TODO: declare mapping of your activation function here (or later in child class if you like)\n",
    "        ###>>> start of solution\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        pass\n",
    "\n",
    "    def forward(self, x, isTraining):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   InputLayer class\n",
    "#------------------------------------------------------------------------------\n",
    "class InputLayer(Layer):\n",
    "    def __init__(self, nX):\n",
    "        super().__init__(act=\"linear\")\n",
    "\n",
    "        self.nX = nX\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        self.shape = (self.nX, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "#------------------------------------------------------------------------------\n",
    "#   Basic Dense Layer class\n",
    "#------------------------------------------------------------------------------\n",
    "class DenseLayer(Layer):\n",
    "    def __init__(self, nUnits, act=\"linear\"):\n",
    "        super().__init__(act)\n",
    "        ###>>> start of solution\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass\n",
    "\n",
    "    def initialize(self, prevLayer):\n",
    "        ###>>> start of solution\n",
    "        #initialize all neurons\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        ###>>> start of solution\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass\n",
    "    \n",
    "    class Neuron:\n",
    "        \"\"\"Single Neuron used in the Dense Layer\"\"\"\n",
    "        def __init__(self, act=\"linear\"):\n",
    "            ###>>> start of solution\n",
    "            \n",
    "            ###<<< end of solution\n",
    "            pass\n",
    "        def initialize(self, prevLayer):\n",
    "            ###>>> start of solution\n",
    "            \n",
    "            ###<<< end of solution\n",
    "            pass\n",
    "        def forward(self, X):\n",
    "            ###>>> start of solution\n",
    "            \n",
    "            ###<<< end of solution\n",
    "            pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activations\n",
    "\n",
    "Implement three standard activation functions (Linear, ReLU, Sigmoid), which you will use in your implementation of dense layer for logical neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   ActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class ActivationFunction:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def __call__(self, Z):\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   LinearActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class LinearActivationFunction(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        ###>>> start of solution\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass\n",
    "\n",
    "    def derivate(self, Z):\n",
    "        # do not need to implement now\n",
    "        pass\n",
    "    \n",
    "#------------------------------------------------------------------------------\n",
    "#   SigmoidActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class SigmoidActivationFunction(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        ###>>> start of solution\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass\n",
    "\n",
    "    def derivate(self, Z):\n",
    "        # do not need to implement now\n",
    "        pass\n",
    "\n",
    "#------------------------------------------------------------------------------\n",
    "#   RELUActivationFunction class\n",
    "#------------------------------------------------------------------------------\n",
    "class RELUActivationFunction(ActivationFunction):\n",
    "    def __call__(self, Z):\n",
    "        ###>>> start of solution\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass\n",
    "\n",
    "    def derivate(self, Z):\n",
    "        # do not need to implement now\n",
    "        pass\n",
    "    \n",
    "# Activation mapping\n",
    "    \n",
    "MAP_ACTIVATION_FUCTIONS = {\n",
    "    \"linear\": LinearActivationFunction,\n",
    "    \"relu\": RELUActivationFunction,\n",
    "    \"sigmoid\": SigmoidActivationFunction\n",
    "}\n",
    "\n",
    "def CreateActivationFunction(kind):\n",
    "    if (kind in MAP_ACTIVATION_FUCTIONS):\n",
    "        return MAP_ACTIVATION_FUCTIONS[kind]()\n",
    "    raise ValueError(kind, \"Unknown activation function {0}\".format(kind))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model class\n",
    "\n",
    "This is the basic class which holds all of your layers and encapsulate functionality to predict results from your input as a forward pass through all the layers after you create your model and initialize all the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#------------------------------------------------------------------------------\n",
    "#   Model class\n",
    "#------------------------------------------------------------------------------\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.layers = []\n",
    "\n",
    "    def addLayer(self,  layer):\n",
    "        self.layers.append(layer)\n",
    "\n",
    "    def initialize(self):\n",
    "        ###>>> start of solution\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass\n",
    "    \n",
    "    def predict(self, X):\n",
    "        ###>>> start of solution\n",
    "        \n",
    "        ###<<< end of solution\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
