{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "\n",
    "import tarfile\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# import urllib\n",
    "\n",
    "# testfile = urllib.URLopener()\n",
    "# testfile.retrieve(\"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\", \"cifar-100-python.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with tarfile.open(\"cifar-100-python.tar.gz\", 'r|gz') as f:\n",
    "        f.extractall(path=\"./\")\n",
    "        f.close()\n",
    "\n",
    "# Extract cifar-100-python.tar.gz to separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open(os.path.join(\"cifar-100-python\", \"train\"), 'rb'), encoding='latin1')\n",
    "X_train_np = data['data']\n",
    "y_train_np = np.asarray(data['fine_labels'], np.int8)\n",
    "y_super_train_np = np.asarray(data['coarse_labels'], np.int8)\n",
    "data = pickle.load(open(os.path.join('cifar-100-python', 'test'), 'rb'), encoding='latin1')\n",
    "X_test_np = data['data']\n",
    "y_test_np = np.asarray(data['fine_labels'], np.int8)\n",
    "y_super_test_np = np.asarray(data['coarse_labels'], np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reshape\n",
    "X_train_np = X_train_np.reshape(-1, 3, 32, 32)\n",
    "X_test_np = X_test_np.reshape(-1, 3, 32, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_np.shape\n",
    "# sns.countplot(y_train_np)\n",
    "\n",
    "x_max,y_max = 3,3\n",
    "offset = 0\n",
    "\n",
    "# f, axarr = plt.subplots(y_max,x_max)\n",
    "# for y in range(y_max):\n",
    "#     for x in range(x_max):\n",
    "#         axarr[y,x].imshow(X_train_np[y*y_max+x+offset])\n",
    "# print(y_train[i])\n",
    "# plt.imshow(np.rollaxis(X_train[i], 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean = X_train_np.mean(axis=(0,1,2), keepdims=True).astype(np.float32)\n",
    "std = X_train_np.std(axis=(0,1,2), keepdims=True).astype(np.float32)\n",
    "\n",
    "X_train_np = (X_train_np - mean) / std\n",
    "X_test_np = (X_test_np - mean) / std\n",
    "\n",
    "#TODO POROVNAT S -MEAN)/MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_v2(X_train,y_train,test_size,random_state=None,sample_limit=None):\n",
    "    total_count = len(X_train)\n",
    "    if sample_limit:\n",
    "        _, X_train, _, y_train = train_test_split(X_train, y_train, test_size=1/(total_count/sample_limit), random_state=random_state)\n",
    "    return train_test_split(X_train, y_train, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41666, 3, 32, 32)\n",
      "(8334, 3, 32, 32)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "X_train,X_valid,y_train,y_valid= train_test_split_v2(X_train_np, y_train_np,sample_limit=None, test_size=1/6, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_train.shape[0]+X_valid.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR(Dataset):\n",
    "    def __init__(self, X,y,transform=None):\n",
    "        self.X=X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = np.rollaxis(self.X[index],0,3)\n",
    "        label = self.y[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(32)\n",
    "])\n",
    "\n",
    "train_data = CIFAR(X_train,y_train, transform=transform)\n",
    "valid_data = CIFAR(X_valid,y_valid, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=256)\n",
    "valid_loader = DataLoader(dataset=valid_data,batch_size=256)\n",
    "\n",
    "\n",
    "# dataiter=iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# print('images shape: ', images.size())\n",
    "# print('labels shape: ', labels.size())\n",
    "\n",
    "\n",
    "# x_max,y_max = 3,3\n",
    "# offset = 0\n",
    "\n",
    "# f, axarr = plt.subplots(y_max,x_max)\n",
    "# for y in range(y_max):\n",
    "#     for x in range(x_max):\n",
    "#         axarr[y,x].imshow(images[y*y_max+x].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11 = nn.Sequential(\n",
    "            nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "\n",
    "vgg11_bn = nn.Sequential(\n",
    "            nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "#             nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(True),\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "#             nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "#             nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(True)\n",
    "        )\n",
    "\n",
    "# vgg13_bn = nn.Sequential(\n",
    "#     nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "# )\n",
    "\n",
    "# vgg16_bn = nn.Sequential(\n",
    "#     nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "# )\n",
    "\n",
    "# vgg19_bn = nn.Sequential(\n",
    "#     nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_chanels, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_chanels,128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(8192,),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256,num_classes),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_chanels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = vgg11_bn\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512,2042),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2042,2042),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2042,num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate=1e-4\n",
    "betas = (0.9,0.999)\n",
    "batch_size = 128\n",
    "sample_limit = 6000\n",
    "epochs = 50\n",
    "\n",
    "n_epochs_stop = 6\n",
    "epochs_no_improve = 0\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model saving\n",
    "\n",
    "def save_model(epoch, model, optimizer, train_loss, valid_loss, accuracy):\n",
    "    PATH = 'current_state.pt'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'valid_loss': valid_loss,\n",
    "        'accuracy' : accuracy\n",
    "    }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">winter-bee-268</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/1axcjyld\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/1axcjyld</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210413_002839-1axcjyld</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 4.5858, valid loss: 4.5525, valid acc: 2.00%\n",
      "Epoch: 2, train loss: 4.4385, valid loss: 4.5106, valid acc: 1.90%\n",
      "Epoch: 3, train loss: 4.2969, valid loss: 4.6198, valid acc: 2.30%\n",
      "Epoch: 4, train loss: 4.1825, valid loss: 4.6031, valid acc: 3.00%\n",
      "Epoch: 5, train loss: 4.1112, valid loss: 4.6315, valid acc: 4.60%\n",
      "Epoch: 6, train loss: 4.0176, valid loss: 4.5966, valid acc: 4.40%\n",
      "Epoch: 7, train loss: 3.8916, valid loss: 4.5874, valid acc: 5.30%\n",
      "Epoch: 8, train loss: 3.8247, valid loss: 4.4556, valid acc: 7.00%\n",
      "Epoch: 9, train loss: 3.7351, valid loss: 4.4104, valid acc: 6.80%\n",
      "Epoch: 10, train loss: 3.6738, valid loss: 4.3468, valid acc: 7.40%\n",
      "Epoch: 11, train loss: 3.6018, valid loss: 4.3403, valid acc: 7.00%\n",
      "Epoch: 12, train loss: 3.5324, valid loss: 4.2951, valid acc: 7.00%\n",
      "Epoch: 13, train loss: 3.4674, valid loss: 4.1881, valid acc: 7.60%\n",
      "Epoch: 14, train loss: 3.3936, valid loss: 4.2035, valid acc: 8.60%\n",
      "Epoch: 15, train loss: 3.3322, valid loss: 4.1831, valid acc: 7.80%\n",
      "Epoch: 16, train loss: 3.2609, valid loss: 4.1546, valid acc: 9.50%\n",
      "Epoch: 17, train loss: 3.1886, valid loss: 4.1825, valid acc: 9.30%\n",
      "Epoch: 18, train loss: 3.1363, valid loss: 4.1973, valid acc: 9.90%\n",
      "Epoch: 19, train loss: 3.0694, valid loss: 4.1049, valid acc: 9.90%\n",
      "Epoch: 20, train loss: 3.0057, valid loss: 3.9132, valid acc: 10.60%\n",
      "Epoch: 21, train loss: 2.9597, valid loss: 4.0282, valid acc: 10.60%\n",
      "Epoch: 22, train loss: 2.9085, valid loss: 4.0483, valid acc: 11.30%\n",
      "Epoch: 23, train loss: 2.8397, valid loss: 3.8646, valid acc: 13.40%\n",
      "Epoch: 24, train loss: 2.7838, valid loss: 4.0982, valid acc: 11.00%\n",
      "Epoch: 25, train loss: 2.7338, valid loss: 3.8667, valid acc: 12.90%\n",
      "Epoch: 26, train loss: 2.7480, valid loss: 3.9920, valid acc: 11.80%\n",
      "Epoch: 27, train loss: 2.6004, valid loss: 3.9186, valid acc: 12.80%\n",
      "Epoch: 28, train loss: 2.5380, valid loss: 3.9115, valid acc: 14.20%\n",
      "Epoch: 29, train loss: 2.4482, valid loss: 4.0791, valid acc: 13.80%\n",
      "Early Stopping on epoch 28/50, min valid loss: 3.8645560145378113, last valid loss: 4.079131931066513\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17496<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210413_002839-1axcjyld\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210413_002839-1axcjyld\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train mean loss</td><td>2.44824</td></tr><tr><td>Valid mean loss</td><td>4.07913</td></tr><tr><td>Accuracy</td><td>13.8</td></tr><tr><td>_runtime</td><td>150</td></tr><tr><td>_timestamp</td><td>1618266669</td></tr><tr><td>_step</td><td>28</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train mean loss</td><td>██▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>Valid mean loss</td><td>▇▇█████▆▆▅▅▅▄▄▄▄▄▄▃▁▂▃▁▃▁▂▁▁▃</td></tr><tr><td>Accuracy</td><td>▁▁▁▂▃▂▃▄▄▄▄▄▄▅▄▅▅▆▆▆▆▆█▆▇▇▇██</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▇▇▇▇▇██</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">winter-bee-268</strong>: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/1axcjyld\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/1axcjyld</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='Zadanie2-CIFAR', entity='xpetricko')\n",
    "run.name = f\"3run-{epochs}e-{batch_size}bs-vgg11_bn\"\n",
    "\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = learning_rate\n",
    "config.sample_limit=sample_limit\n",
    "config.batch_size=batch_size\n",
    "config.epochs_with_no_improve =  6\n",
    "config.notes='droput added'\n",
    "\n",
    "\n",
    "model = Net(3,100)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=learning_rate) # pri adamW weight_decay = 1e-2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split_v2(X_train_np,\n",
    "                                                      y_train_np,\n",
    "                                                      sample_limit=sample_limit,\n",
    "                                                      test_size=1/6,\n",
    "                                                      random_state=7)\n",
    "\n",
    "train_data = CIFAR(X_train,y_train, transform=transforms.ToTensor())\n",
    "valid_data = CIFAR(X_valid,y_valid, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size)\n",
    "valid_loader = DataLoader(dataset=valid_data,batch_size=batch_size)\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_valid_losses = []\n",
    "valid_acc_list = []\n",
    "min_valid_loss = np.Inf\n",
    "\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(images)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,(images,labels) in enumerate(valid_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model.forward(images)\n",
    "            loss = loss_fn(output,labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(output.data,1)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    accuracy = 100*correct/total\n",
    "    valid_acc_list.append(accuracy)\n",
    "      \n",
    "\n",
    "    wandb.log({\"Train mean loss\":mean_train_losses[-1],\"Valid mean loss\":mean_valid_losses[-1],\"Accuracy\":accuracy})\n",
    "\n",
    "    print('Epoch: {}, train loss: {:.4f}, valid loss: {:.4f}, valid acc: {:.2f}%'.format(epoch+1,mean_train_losses[-1],mean_valid_losses[-1],accuracy))\n",
    "    \n",
    "    if(mean_valid_losses[-1] < min_valid_loss):\n",
    "        save_model(epoch, model, optimizer, mean_train_losses[-1], mean_valid_losses[-1], accuracy)\n",
    "        epochs_no_improve = 0\n",
    "        min_valid_loss = mean_valid_losses[-1]\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epoch > 5 and epochs_no_improve == n_epochs_stop:\n",
    "        print(f'Early Stopping on epoch {epoch}/{epochs}, min valid loss: {min_valid_loss}, last valid loss: {mean_valid_losses[-1]}')\n",
    "#         early_stop = True\n",
    "        break\n",
    "torch.cuda.empty_cache()        \n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  321359 KB |    1290 MB |    2740 GB |    2740 GB |\n",
      "|       from large pool |  314283 KB |    1282 MB |    2734 GB |    2734 GB |\n",
      "|       from small pool |    7075 KB |      11 MB |       5 GB |       5 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  321359 KB |    1290 MB |    2740 GB |    2740 GB |\n",
      "|       from large pool |  314283 KB |    1282 MB |    2734 GB |    2734 GB |\n",
      "|       from small pool |    7075 KB |      11 MB |       5 GB |       5 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  589824 KB |    1950 MB |    1950 MB |    1374 MB |\n",
      "|       from large pool |  581632 KB |    1936 MB |    1936 MB |    1368 MB |\n",
      "|       from small pool |    8192 KB |      14 MB |      14 MB |       6 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  268465 KB |  660841 KB |    2825 GB |    2825 GB |\n",
      "|       from large pool |  267348 KB |  658301 KB |    2818 GB |    2817 GB |\n",
      "|       from small pool |    1116 KB |    5203 KB |       7 GB |       7 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     258    |     339    |  168678    |  168420    |\n",
      "|       from large pool |      45    |      91    |   78753    |   78708    |\n",
      "|       from small pool |     213    |     248    |   89925    |   89712    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     258    |     339    |  168678    |  168420    |\n",
      "|       from large pool |      45    |      91    |   78753    |   78708    |\n",
      "|       from small pool |     213    |     248    |   89925    |   89712    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      13    |      20    |      20    |       7    |\n",
      "|       from large pool |       9    |      13    |      13    |       4    |\n",
      "|       from small pool |       4    |       7    |       7    |       3    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      21    |      28    |   88233    |   88212    |\n",
      "|       from large pool |       7    |      10    |   36191    |   36184    |\n",
      "|       from small pool |      14    |      20    |   52042    |   52028    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# trfs = [\n",
    "#     # {\"name\":\"No transform\", \"tr\":transforms.ToTensor()},\n",
    "#     # {\n",
    "#     #     \"name\":\"Normalize\",\n",
    "#     #     \"tr\": transforms.Compose([\n",
    "#     #         transforms.ToTensor(),\n",
    "#     #         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     #     ]),\n",
    "#     #     \"tr_v\": transforms.Compose([\n",
    "#     #         transforms.ToTensor(),\n",
    "#     #         transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "#     #     ])\n",
    "#     # },\n",
    "#     {\n",
    "#         \"name\":\"Random crop\",\n",
    "#         \"tr\":transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.RandomResizedCrop(32)\n",
    "#         ]),\n",
    "#         \"tr_v\": transforms.Compose([\n",
    "#             transforms.ToTensor()\n",
    "#         ])\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\":\"Norm + Crop\",\n",
    "#         \"tr\":transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#             transforms.RandomResizedCrop(32)\n",
    "#         ]),\n",
    "#         \"tr_v\": transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "#         ])\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\":\"RandFlip + Norm + Crop\",\n",
    "#         \"tr\":transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#             transforms.RandomResizedCrop(32)\n",
    "#         ]),\n",
    "#         \"tr_v\": transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "#         ])\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# MULTITASK LEARNING PART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR_Multitask(Dataset):\n",
    "    def __init__(self, X,y, y_super,transform=None):\n",
    "        self.X=X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.y_super = torch.LongTensor(y_super)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = np.rollaxis(self.X[index],0,3)\n",
    "        label = self.y[index]\n",
    "        super_label = self.y_super[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, super_label\n",
    "\n",
    "\n",
    "class NetMultitask(nn.Module):\n",
    "    def __init__(self, in_chanels, num_classes,num_super_classes):\n",
    "        super(NetMultitask, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_chanels,64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(True),\n",
    "            # ---\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            # ---\n",
    "#             nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(True),\n",
    "            # ---\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            # ---\n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(True),\n",
    "            # ---\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "            # ---\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            # ---\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            # ---\n",
    "#             nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(True),\n",
    "            # ---\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.Dropout(),\n",
    "            # ---\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            # ---\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(True)\n",
    "            # ---------------------\n",
    "#             nn.Conv2d(in_chanels,32, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(32),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             # ---\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(32,64, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(64),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             nn.Dropout(inplace=True, p=0.2),\n",
    "#             # ---\n",
    "# #             nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
    "# #             nn.BatchNorm2d(64),\n",
    "# #             nn.LeakyReLU(True),\n",
    "#             # ---\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             nn.Dropout(inplace=True, p=0.2),\n",
    "#             # ---\n",
    "#             nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             # ---\n",
    "#             nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             # ---\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             nn.Dropout(inplace=True, p=0.2),\n",
    "#             # ---\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             # ---\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             # ---\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             nn.Dropout(inplace=True, p=0.2),\n",
    "#             # ---\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "#             # ---\n",
    "#             nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048,2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048,num_classes),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "        self.super_clasifier = nn.Sequential(\n",
    "            nn.Linear(512,2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048,2048),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2048,num_super_classes),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_c = self.classifier(x)\n",
    "        x_sc = self.super_clasifier(x)\n",
    "        return x_c,x_sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate=1e-4\n",
    "betas = (0.9,0.999)\n",
    "batch_size = 128\n",
    "sample_limit = 6000\n",
    "epochs = 50\n",
    "\n",
    "n_epochs_stop = 6\n",
    "epochs_no_improve = 0\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">sleek-frog-269</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/3aymnzn0\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/3aymnzn0</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210413_004110-3aymnzn0</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\netenv\\lib\\site-packages\\torch\\nn\\modules\\container.py:119: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 7.5814, valid loss: 4.5984, valid acc: 1.90%\n",
      "Epoch: 2, train loss: 7.5408, valid loss: 4.5997, valid acc: 1.60%\n",
      "Epoch: 3, train loss: 7.5204, valid loss: 4.5974, valid acc: 2.70%\n",
      "Epoch: 4, train loss: 7.5071, valid loss: 4.5958, valid acc: 2.20%\n",
      "Epoch: 5, train loss: 7.4928, valid loss: 4.5971, valid acc: 2.00%\n",
      "Epoch: 6, train loss: 7.4855, valid loss: 4.5880, valid acc: 3.20%\n",
      "Epoch: 7, train loss: 7.4704, valid loss: 4.5920, valid acc: 2.60%\n",
      "Epoch: 8, train loss: 7.4599, valid loss: 4.5871, valid acc: 3.00%\n",
      "Epoch: 9, train loss: 7.4441, valid loss: 4.5811, valid acc: 3.70%\n",
      "Epoch: 10, train loss: 7.4289, valid loss: 4.5843, valid acc: 3.50%\n",
      "Epoch: 11, train loss: 7.4181, valid loss: 4.5767, valid acc: 4.60%\n",
      "Epoch: 12, train loss: 7.4002, valid loss: 4.5788, valid acc: 4.20%\n",
      "Epoch: 13, train loss: 7.4065, valid loss: 4.5771, valid acc: 4.50%\n",
      "Epoch: 14, train loss: 7.3874, valid loss: 4.5811, valid acc: 4.00%\n",
      "Epoch: 15, train loss: 7.3821, valid loss: 4.5782, valid acc: 4.30%\n",
      "Epoch: 16, train loss: 7.3753, valid loss: 4.5783, valid acc: 4.40%\n",
      "Epoch: 17, train loss: 7.3656, valid loss: 4.5727, valid acc: 4.90%\n",
      "Epoch: 18, train loss: 7.3949, valid loss: 4.5739, valid acc: 4.80%\n",
      "Epoch: 19, train loss: 7.3593, valid loss: 4.5651, valid acc: 5.40%\n",
      "Epoch: 20, train loss: 7.3563, valid loss: 4.5639, valid acc: 5.70%\n",
      "Epoch: 21, train loss: 7.3380, valid loss: 4.5701, valid acc: 5.20%\n",
      "Epoch: 22, train loss: 7.3391, valid loss: 4.5772, valid acc: 4.20%\n",
      "Epoch: 23, train loss: 7.3169, valid loss: 4.5727, valid acc: 4.60%\n",
      "Epoch: 24, train loss: 7.3125, valid loss: 4.5645, valid acc: 5.60%\n",
      "Epoch: 25, train loss: 7.3124, valid loss: 4.5614, valid acc: 6.30%\n",
      "Epoch: 26, train loss: 7.3123, valid loss: 4.5606, valid acc: 6.00%\n",
      "Epoch: 27, train loss: 7.3064, valid loss: 4.5569, valid acc: 6.40%\n",
      "Epoch: 28, train loss: 7.2998, valid loss: 4.5634, valid acc: 5.70%\n",
      "Epoch: 29, train loss: 7.2856, valid loss: 4.5658, valid acc: 5.30%\n",
      "Epoch: 30, train loss: 7.2838, valid loss: 4.5564, valid acc: 6.30%\n",
      "Epoch: 31, train loss: 7.2744, valid loss: 4.5597, valid acc: 6.20%\n",
      "Epoch: 32, train loss: 7.2804, valid loss: 4.5473, valid acc: 7.60%\n",
      "Epoch: 33, train loss: 7.2908, valid loss: 4.5593, valid acc: 6.40%\n",
      "Epoch: 34, train loss: 7.2695, valid loss: 4.5467, valid acc: 7.60%\n",
      "Epoch: 35, train loss: 7.2621, valid loss: 4.5495, valid acc: 7.40%\n",
      "Epoch: 36, train loss: 7.2424, valid loss: 4.5439, valid acc: 7.80%\n",
      "Epoch: 37, train loss: 7.2466, valid loss: 4.5497, valid acc: 7.30%\n",
      "Epoch: 38, train loss: 7.2384, valid loss: 4.5313, valid acc: 9.30%\n",
      "Epoch: 39, train loss: 7.2311, valid loss: 4.5404, valid acc: 8.40%\n",
      "Epoch: 40, train loss: 7.2335, valid loss: 4.5574, valid acc: 6.40%\n",
      "Epoch: 41, train loss: 7.2384, valid loss: 4.5462, valid acc: 7.30%\n",
      "Epoch: 42, train loss: 7.2217, valid loss: 4.5458, valid acc: 7.40%\n",
      "Epoch: 43, train loss: 7.2121, valid loss: 4.5388, valid acc: 8.70%\n",
      "Epoch: 44, train loss: 7.2110, valid loss: 4.5557, valid acc: 6.60%\n",
      "Early Stopping on epoch 43/50, min valid loss: 4.53130042552948, last valid loss: 4.555749475955963\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 30756<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210413_004110-3aymnzn0\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210413_004110-3aymnzn0\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train mean loss</td><td>7.21098</td></tr><tr><td>Valid mean loss</td><td>4.55575</td></tr><tr><td>Accuracy</td><td>6.6</td></tr><tr><td>_runtime</td><td>244</td></tr><tr><td>_timestamp</td><td>1618267515</td></tr><tr><td>_step</td><td>43</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train mean loss</td><td>█▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▁▁▂▁▁</td></tr><tr><td>Valid mean loss</td><td>█████▇▇▇▆▆▆▆▆▆▆▅▅▄▄▅▅▄▄▄▄▄▅▄▄▃▃▃▂▃▁▂▄▃▂▄</td></tr><tr><td>Accuracy</td><td>▁▁▂▂▁▂▂▂▃▃▃▄▃▃▄▄▄▄▅▄▄▅▅▅▅▅▄▅▅▆▆▆▇▆█▇▅▆▆▆</td></tr><tr><td>_runtime</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">sleek-frog-269</strong>: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/3aymnzn0\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/3aymnzn0</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='Zadanie2-CIFAR', entity='xpetricko')\n",
    "run.name = f\"Multitask run 9-{epochs}e-{batch_size}bs-vgg11_bn\"\n",
    "\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = learning_rate\n",
    "config.sample_limit=sample_limit\n",
    "config.batch_size=batch_size\n",
    "config.epochs_with_no_improve = n_epochs_stop\n",
    "config.notes='upd arch'\n",
    "\n",
    "\n",
    "model = NetMultitask(3,100,20)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=learning_rate, weight_decay = 0.1) # pri adamW weight_decay = 1e-2\n",
    "#optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.2, nesterov=True, weight_decay = 0.1, dampening=0)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split_v2(X_train_np,\n",
    "                                                      y_train_np,\n",
    "                                                      sample_limit=sample_limit,\n",
    "                                                      test_size=1/6,\n",
    "                                                      random_state=7)\n",
    "\n",
    "_,_,y_super_train,y_super_valid = train_test_split_v2(X_train_np,\n",
    "                                                      y_super_train_np,\n",
    "                                                      sample_limit=sample_limit,\n",
    "                                                      test_size=1/6,\n",
    "                                                      random_state=7)\n",
    "\n",
    "train_data = CIFAR_Multitask(X_train,y_train,y_super_train, transform=transforms.ToTensor())\n",
    "valid_data = CIFAR(X_valid,y_valid, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size)\n",
    "valid_loader = DataLoader(dataset=valid_data,batch_size=batch_size)\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_valid_losses = []\n",
    "valid_acc_list = []\n",
    "min_valid_loss = np.Inf\n",
    "\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for i, (images,labels, super_labels) in enumerate(train_loader):\n",
    "        images, labels, super_labels = images.to(device), labels.to(device), super_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, super_output = model.forward(images)\n",
    "        loss1 = loss_fn(output,labels)\n",
    "        loss2 = loss_fn(super_output,super_labels)\n",
    "        loss= loss1+loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,(images,labels) in enumerate(valid_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output,_ = model.forward(images)\n",
    "            loss = loss_fn(output,labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(output.data,1)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    accuracy = 100*correct/total\n",
    "    valid_acc_list.append(accuracy)\n",
    "\n",
    "\n",
    "    wandb.log({\"Train mean loss\":mean_train_losses[-1],\"Valid mean loss\":mean_valid_losses[-1],\"Accuracy\":accuracy})\n",
    "\n",
    "    print('Epoch: {}, train loss: {:.4f}, valid loss: {:.4f}, valid acc: {:.2f}%'.format(epoch+1,mean_train_losses[-1],mean_valid_losses[-1],accuracy))\n",
    "\n",
    "    if(mean_valid_losses[-1] < min_valid_loss):\n",
    "        save_model(epoch, model, optimizer, mean_train_losses[-1], mean_valid_losses[-1], accuracy)\n",
    "        epochs_no_improve = 0\n",
    "        min_valid_loss = mean_valid_losses[-1]\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epoch > 5 and epochs_no_improve == n_epochs_stop:\n",
    "        print(f'Early Stopping on epoch {epoch}/{epochs}, min valid loss: {min_valid_loss}, last valid loss: {mean_valid_losses[-1]}')\n",
    "#         early_stop = True\n",
    "        break\n",
    "torch.cuda.empty_cache()\n",
    "run.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netenv",
   "language": "python",
   "name": "netenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
