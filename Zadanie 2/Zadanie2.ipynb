{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "\n",
    "import tarfile\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# import requests\n",
    "\n",
    "# import urllib\n",
    "\n",
    "# testfile = urllib.URLopener()\n",
    "# testfile.retrieve(\"https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\", \"cifar-100-python.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "with tarfile.open(\"cifar-100-python.tar.gz\", 'r|gz') as f:\n",
    "        f.extractall(path=\"./\")\n",
    "        f.close()\n",
    "\n",
    "# Extract cifar-100-python.tar.gz to separate data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pickle.load(open(os.path.join(\"cifar-100-python\", \"train\"), 'rb'), encoding='latin1')\n",
    "X_train_np = data['data']\n",
    "y_train_np = np.asarray(data['fine_labels'], np.int8)\n",
    "y_super_train_np = np.asarray(data['coarse_labels'], np.int8)\n",
    "data = pickle.load(open(os.path.join('cifar-100-python', 'test'), 'rb'), encoding='latin1')\n",
    "X_test_np = data['data']\n",
    "y_test_np = np.asarray(data['fine_labels'], np.int8)\n",
    "y_super_test_np = np.asarray(data['coarse_labels'], np.int8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# reshape\n",
    "X_train_np = X_train_np.reshape(-1, 3, 32, 32)\n",
    "X_test_np = X_test_np.reshape(-1, 3, 32, 32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_np.shape\n",
    "# sns.countplot(y_train_np)\n",
    "\n",
    "x_max,y_max = 3,3\n",
    "offset = 0\n",
    "\n",
    "# f, axarr = plt.subplots(y_max,x_max)\n",
    "# for y in range(y_max):\n",
    "#     for x in range(x_max):\n",
    "#         axarr[y,x].imshow(X_train_np[y*y_max+x+offset])\n",
    "# print(y_train[i])\n",
    "# plt.imshow(np.rollaxis(X_train[i], 0, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "mean = X_train_np.mean(axis=(0,1,2), keepdims=True).astype(np.float32)\n",
    "std = X_train_np.std(axis=(0,1,2), keepdims=True).astype(np.float32)\n",
    "\n",
    "X_train_np = (X_train_np - mean) / std\n",
    "X_test_np = (X_test_np - mean) / std\n",
    "\n",
    "#TODO POROVNAT S -MEAN)/MAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split_v2(X_train,y_train,test_size,random_state=None,sample_limit=None):\n",
    "    total_count = len(X_train)\n",
    "    if sample_limit:\n",
    "        _, X_train, _, y_train = train_test_split(X_train, y_train, test_size=1/(total_count/sample_limit), random_state=random_state)\n",
    "    return train_test_split(X_train, y_train, test_size=test_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41666, 3, 32, 32)\n",
      "(8334, 3, 32, 32)\n",
      "50000\n"
     ]
    }
   ],
   "source": [
    "X_train,X_valid,y_train,y_valid= train_test_split_v2(X_train_np, y_train_np,sample_limit=None, test_size=1/6, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_valid.shape)\n",
    "print(X_train.shape[0]+X_valid.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class CIFAR(Dataset):\n",
    "    def __init__(self, X,y,transform=None):\n",
    "        self.X=X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = np.rollaxis(self.X[index],0,3)\n",
    "        label = self.y[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.RandomResizedCrop(32)\n",
    "])\n",
    "\n",
    "train_data = CIFAR(X_train,y_train, transform=transform)\n",
    "valid_data = CIFAR(X_valid,y_valid, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=256)\n",
    "valid_loader = DataLoader(dataset=valid_data,batch_size=256)\n",
    "\n",
    "\n",
    "# dataiter=iter(train_loader)\n",
    "# images, labels = dataiter.next()\n",
    "\n",
    "# print('images shape: ', images.size())\n",
    "# print('labels shape: ', labels.size())\n",
    "\n",
    "\n",
    "# x_max,y_max = 3,3\n",
    "# offset = 0\n",
    "\n",
    "# f, axarr = plt.subplots(y_max,x_max)\n",
    "# for y in range(y_max):\n",
    "#     for x in range(x_max):\n",
    "#         axarr[y,x].imshow(images[y*y_max+x].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg11 = nn.Sequential(\n",
    "            nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(True),\n",
    "        )\n",
    "\n",
    "\n",
    "vgg11_bn = nn.Sequential(\n",
    "            nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(True),\n",
    "#             nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(True),\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "#             nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True)\n",
    "        )\n",
    "\n",
    "# vgg13_bn = nn.Sequential(\n",
    "#     nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "# )\n",
    "\n",
    "# vgg16_bn = nn.Sequential(\n",
    "#     nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "# )\n",
    "\n",
    "# vgg19_bn = nn.Sequential(\n",
    "#     nn.Conv2d(3,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(64,64, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(64),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(128),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(256),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True),\n",
    "#     # ---\n",
    "#     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#     nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#     nn.BatchNorm2d(512),\n",
    "#     nn.ReLU(True)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, in_chanels, num_classes):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_chanels,128, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(8192,),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(512,256),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256,num_classes),\n",
    "            nn.Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_chanels, num_classes):\n",
    "        super(Net, self).__init__()\n",
    "        self.features = vgg11_bn\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512,2042),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2042,2042),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2042,num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "learning_rate=2e-5\n",
    "betas = (0.9,0.999)\n",
    "batch_size = 256\n",
    "sample_limit = 6000\n",
    "epochs = 50\n",
    "\n",
    "n_epochs_stop = 6\n",
    "epochs_no_improve = 0\n",
    "early_stop = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model saving\n",
    "\n",
    "def save_model(epoch, model, optimizer, train_loss, valid_loss, accuracy):\n",
    "    PATH = 'current_state.pt'\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'valid_loss': valid_loss,\n",
    "        'accuracy' : accuracy\n",
    "    }, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.22<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">stellar-eon-178</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/3gsjrb1g\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/3gsjrb1g</a><br/>\n",
       "                Run data is saved locally in <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210410_215025-3gsjrb1g</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 4.5834, valid loss: 4.5974, valid acc: 1.70%\n",
      "Epoch: 2, train loss: 4.4944, valid loss: 4.5073, valid acc: 8.00%\n",
      "Epoch: 3, train loss: 4.4140, valid loss: 4.4007, valid acc: 8.00%\n",
      "Epoch: 4, train loss: 4.3360, valid loss: 4.3260, valid acc: 8.10%\n",
      "Epoch: 5, train loss: 4.2536, valid loss: 4.2640, valid acc: 9.50%\n",
      "Epoch: 6, train loss: 4.1677, valid loss: 4.2011, valid acc: 10.70%\n",
      "Epoch: 7, train loss: 4.0927, valid loss: 4.1375, valid acc: 12.00%\n",
      "Epoch: 8, train loss: 3.9995, valid loss: 4.0646, valid acc: 12.90%\n",
      "Epoch: 9, train loss: 3.8978, valid loss: 3.9887, valid acc: 14.10%\n",
      "Epoch: 10, train loss: 3.7946, valid loss: 3.9246, valid acc: 14.70%\n",
      "Epoch: 11, train loss: 3.6878, valid loss: 3.8564, valid acc: 14.90%\n",
      "Epoch: 12, train loss: 3.5636, valid loss: 3.7943, valid acc: 14.90%\n",
      "Epoch: 13, train loss: 3.4416, valid loss: 3.7352, valid acc: 15.90%\n",
      "Epoch: 14, train loss: 3.2896, valid loss: 3.6853, valid acc: 16.50%\n",
      "Epoch: 15, train loss: 3.1301, valid loss: 3.6334, valid acc: 16.80%\n",
      "Epoch: 16, train loss: 2.9422, valid loss: 3.5976, valid acc: 16.80%\n",
      "Epoch: 17, train loss: 2.7578, valid loss: 3.5389, valid acc: 18.30%\n",
      "Epoch: 18, train loss: 2.5395, valid loss: 3.5205, valid acc: 17.40%\n",
      "Epoch: 19, train loss: 2.3202, valid loss: 3.4921, valid acc: 18.10%\n",
      "Epoch: 20, train loss: 2.0885, valid loss: 3.4786, valid acc: 17.50%\n",
      "Epoch: 21, train loss: 1.8750, valid loss: 3.5047, valid acc: 18.00%\n",
      "Epoch: 22, train loss: 1.6931, valid loss: 3.5807, valid acc: 17.70%\n",
      "Epoch: 23, train loss: 1.5267, valid loss: 3.5431, valid acc: 18.30%\n",
      "Epoch: 24, train loss: 1.3331, valid loss: 3.4771, valid acc: 19.20%\n",
      "Epoch: 25, train loss: 1.1632, valid loss: 3.5500, valid acc: 18.30%\n",
      "Epoch: 26, train loss: 1.0200, valid loss: 3.6341, valid acc: 16.80%\n",
      "Epoch: 27, train loss: 0.8557, valid loss: 3.5712, valid acc: 16.90%\n",
      "Epoch: 28, train loss: 0.7171, valid loss: 3.6848, valid acc: 16.30%\n",
      "Epoch: 29, train loss: 0.5975, valid loss: 3.7007, valid acc: 17.00%\n",
      "Epoch: 30, train loss: 0.5090, valid loss: 3.7802, valid acc: 15.50%\n",
      "Early Stopping on epoch 29/50, min valid loss: 3.4770528078079224, last valid loss: 3.780240476131439\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<br/>Waiting for W&B process to finish, PID 17784<br/>Program ended successfully."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find user logs for this run at: <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210410_215025-3gsjrb1g\\logs\\debug.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find internal logs for this run at: <code>C:\\Users\\ollyt\\Jupyter-projects\\NSIETE\\Zadanie 2\\wandb\\run-20210410_215025-3gsjrb1g\\logs\\debug-internal.log</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run summary:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train mean loss</td><td>0.50897</td></tr><tr><td>Valid mean loss</td><td>3.78024</td></tr><tr><td>Accuracy</td><td>15.5</td></tr><tr><td>_runtime</td><td>132</td></tr><tr><td>_timestamp</td><td>1618084358</td></tr><tr><td>_step</td><td>29</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<h3>Run history:</h3><br/><style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
       "    </style><table class=\"wandb\">\n",
       "<tr><td>Train mean loss</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▄▄▄▃▃▃▂▂▂▂▁▁▁</td></tr><tr><td>Valid mean loss</td><td>█▇▇▆▆▆▅▅▄▄▃▃▃▂▂▂▁▁▁▁▁▂▁▁▁▂▂▂▂▃</td></tr><tr><td>Accuracy</td><td>▁▄▄▄▄▅▅▅▆▆▆▆▇▇▇▇█▇█▇█▇███▇▇▇▇▇</td></tr><tr><td>_runtime</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▂▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr></table><br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                    <br/>Synced <strong style=\"color:#cdcd00\">stellar-eon-178</strong>: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/3gsjrb1g\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/3gsjrb1g</a><br/>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project='Zadanie2-CIFAR', entity='xpetricko')\n",
    "run.name = f\"17run-{epochs}e-{batch_size}bs-vgg11_bn\"\n",
    "\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = learning_rate\n",
    "config.sample_limit=sample_limit\n",
    "config.batch_size=batch_size\n",
    "config.epochs_with_no_improve =  6\n",
    "config.notes='update_architecture'\n",
    "\n",
    "\n",
    "model = Net(3,100)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=learning_rate, weight_decay = 0.1) # pri adamW weight_decay = 1e-2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split_v2(X_train_np,\n",
    "                                                      y_train_np,\n",
    "                                                      sample_limit=sample_limit,\n",
    "                                                      test_size=1/6,\n",
    "                                                      random_state=7)\n",
    "\n",
    "train_data = CIFAR(X_train,y_train, transform=transforms.ToTensor())\n",
    "valid_data = CIFAR(X_valid,y_valid, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size)\n",
    "valid_loader = DataLoader(dataset=valid_data,batch_size=batch_size)\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_valid_losses = []\n",
    "valid_acc_list = []\n",
    "min_valid_loss = np.Inf\n",
    "\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for i, (images,labels) in enumerate(train_loader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model.forward(images)\n",
    "        loss = loss_fn(output,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,(images,labels) in enumerate(valid_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model.forward(images)\n",
    "            loss = loss_fn(output,labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(output.data,1)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    accuracy = 100*correct/total\n",
    "    valid_acc_list.append(accuracy)\n",
    "      \n",
    "\n",
    "    wandb.log({\"Train mean loss\":mean_train_losses[-1],\"Valid mean loss\":mean_valid_losses[-1],\"Accuracy\":accuracy})\n",
    "\n",
    "    print('Epoch: {}, train loss: {:.4f}, valid loss: {:.4f}, valid acc: {:.2f}%'.format(epoch+1,mean_train_losses[-1],mean_valid_losses[-1],accuracy))\n",
    "    \n",
    "    if(mean_valid_losses[-1] < min_valid_loss):\n",
    "        save_model(epoch, model, optimizer, mean_train_losses[-1], mean_valid_losses[-1], accuracy)\n",
    "        epochs_no_improve = 0\n",
    "        min_valid_loss = mean_valid_losses[-1]\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "    \n",
    "    if epoch > 5 and epochs_no_improve == n_epochs_stop:\n",
    "        print(f'Early Stopping on epoch {epoch}/{epochs}, min valid loss: {min_valid_loss}, last valid loss: {mean_valid_losses[-1]}')\n",
    "#         early_stop = True\n",
    "        break\n",
    "torch.cuda.empty_cache()        \n",
    "run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  321359 KB |    1290 MB |    2740 GB |    2740 GB |\n",
      "|       from large pool |  314283 KB |    1282 MB |    2734 GB |    2734 GB |\n",
      "|       from small pool |    7075 KB |      11 MB |       5 GB |       5 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  321359 KB |    1290 MB |    2740 GB |    2740 GB |\n",
      "|       from large pool |  314283 KB |    1282 MB |    2734 GB |    2734 GB |\n",
      "|       from small pool |    7075 KB |      11 MB |       5 GB |       5 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  589824 KB |    1950 MB |    1950 MB |    1374 MB |\n",
      "|       from large pool |  581632 KB |    1936 MB |    1936 MB |    1368 MB |\n",
      "|       from small pool |    8192 KB |      14 MB |      14 MB |       6 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  268465 KB |  660841 KB |    2825 GB |    2825 GB |\n",
      "|       from large pool |  267348 KB |  658301 KB |    2818 GB |    2817 GB |\n",
      "|       from small pool |    1116 KB |    5203 KB |       7 GB |       7 GB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     258    |     339    |  168678    |  168420    |\n",
      "|       from large pool |      45    |      91    |   78753    |   78708    |\n",
      "|       from small pool |     213    |     248    |   89925    |   89712    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     258    |     339    |  168678    |  168420    |\n",
      "|       from large pool |      45    |      91    |   78753    |   78708    |\n",
      "|       from small pool |     213    |     248    |   89925    |   89712    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      13    |      20    |      20    |       7    |\n",
      "|       from large pool |       9    |      13    |      13    |       4    |\n",
      "|       from small pool |       4    |       7    |       7    |       3    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      21    |      28    |   88233    |   88212    |\n",
      "|       from large pool |       7    |      10    |   36191    |   36184    |\n",
      "|       from small pool |      14    |      20    |   52042    |   52028    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# trfs = [\n",
    "#     # {\"name\":\"No transform\", \"tr\":transforms.ToTensor()},\n",
    "#     # {\n",
    "#     #     \"name\":\"Normalize\",\n",
    "#     #     \"tr\": transforms.Compose([\n",
    "#     #         transforms.ToTensor(),\n",
    "#     #         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "#     #     ]),\n",
    "#     #     \"tr_v\": transforms.Compose([\n",
    "#     #         transforms.ToTensor(),\n",
    "#     #         transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "#     #     ])\n",
    "#     # },\n",
    "#     {\n",
    "#         \"name\":\"Random crop\",\n",
    "#         \"tr\":transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.RandomResizedCrop(32)\n",
    "#         ]),\n",
    "#         \"tr_v\": transforms.Compose([\n",
    "#             transforms.ToTensor()\n",
    "#         ])\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\":\"Norm + Crop\",\n",
    "#         \"tr\":transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#             transforms.RandomResizedCrop(32)\n",
    "#         ]),\n",
    "#         \"tr_v\": transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "#         ])\n",
    "#     },\n",
    "#     {\n",
    "#         \"name\":\"RandFlip + Norm + Crop\",\n",
    "#         \"tr\":transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.RandomHorizontalFlip(),\n",
    "#             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "#             transforms.RandomResizedCrop(32)\n",
    "#         ]),\n",
    "#         \"tr_v\": transforms.Compose([\n",
    "#             transforms.ToTensor(),\n",
    "#             transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "#         ])\n",
    "#     }\n",
    "# ]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# MULTITASK LEARNING PART"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "class CIFAR_Multitask(Dataset):\n",
    "    def __init__(self, X,y, y_super,transform=None):\n",
    "        self.X=X\n",
    "        self.y = torch.LongTensor(y)\n",
    "        self.y_super = torch.LongTensor(y_super)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        image = np.rollaxis(self.X[index],0,3)\n",
    "        label = self.y[index]\n",
    "        super_label = self.y_super[index]\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label, super_label\n",
    "\n",
    "\n",
    "class NetMultitask(nn.Module):\n",
    "    def __init__(self, in_chanels, num_classes,num_super_classes):\n",
    "        super(NetMultitask, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_chanels,64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64,128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(True),\n",
    "#             nn.Conv2d(128,128, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(128),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(128,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(True),\n",
    "#             nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(256),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.Conv2d(256,256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(256,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "#             nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "#             nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "#             nn.BatchNorm2d(512),\n",
    "#             nn.LeakyReLU(True),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(512,512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.LeakyReLU(True)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512,2042),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2042,2042),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2042,num_classes)\n",
    "        )\n",
    "\n",
    "        self.super_clasifier = nn.Sequential(\n",
    "            nn.Linear(512,2042),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2042,2042),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(2042,num_super_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x_c = self.classifier(x)\n",
    "        x_sc = self.super_clasifier(x)\n",
    "        return x_c,x_sc"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [],
   "source": [
    "learning_rate=2e-5\n",
    "betas = (0.9,0.999)\n",
    "batch_size = 256\n",
    "sample_limit = 6000\n",
    "epochs = 50\n",
    "\n",
    "n_epochs_stop = 6\n",
    "epochs_no_improve = 0\n",
    "early_stop = False"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:bxuot8sl) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<br/>Waiting for W&B process to finish, PID 30424<br/>Program ended successfully."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "721b2320d4cd4634b71892475d9043d0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find user logs for this run at: <code>D:\\Skola_fiit\\neural_networks_at_fiit\\Zadanie 2\\wandb\\run-20210410_221053-bxuot8sl\\logs\\debug.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find internal logs for this run at: <code>D:\\Skola_fiit\\neural_networks_at_fiit\\Zadanie 2\\wandb\\run-20210410_221053-bxuot8sl\\logs\\debug-internal.log</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Train mean loss</td><td>7.53442</td></tr><tr><td>Valid mean loss</td><td>4.60176</td></tr><tr><td>Accuracy</td><td>0.9</td></tr><tr><td>_runtime</td><td>17</td></tr><tr><td>_timestamp</td><td>1618085473</td></tr><tr><td>_step</td><td>0</td></tr></table>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>Train mean loss</td><td>▁</td></tr><tr><td>Valid mean loss</td><td>▁</td></tr><tr><td>Accuracy</td><td>▁</td></tr><tr><td>_runtime</td><td>▁</td></tr><tr><td>_timestamp</td><td>▁</td></tr><tr><td>_step</td><td>▁</td></tr></table><br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                    <br/>Synced <strong style=\"color:#cdcd00\">breezy-monkey-184</strong>: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/bxuot8sl\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/bxuot8sl</a><br/>\n                "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "...Successfully finished last run (ID:bxuot8sl). Initializing new run:<br/><br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: wandb version 0.10.25 is available!  To upgrade, please run:\n",
      "wandb:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n                Tracking run with wandb version 0.10.24<br/>\n                Syncing run <strong style=\"color:#cdcd00\">amber-pond-185</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR</a><br/>\n                Run page: <a href=\"https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/bb9nmgi7\" target=\"_blank\">https://wandb.ai/xpetricko/Zadanie2-CIFAR/runs/bb9nmgi7</a><br/>\n                Run data is saved locally in <code>D:\\Skola_fiit\\neural_networks_at_fiit\\Zadanie 2\\wandb\\run-20210410_221130-bb9nmgi7</code><br/><br/>\n            "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, train loss: 7.5295, valid loss: 4.6041, valid acc: 1.20%\n",
      "Epoch: 2, train loss: 7.3101, valid loss: 4.5336, valid acc: 4.70%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[1;32m<ipython-input-81-5d1b24d0af5e>\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m     88\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     89\u001B[0m     \u001B[1;32mif\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mmean_valid_losses\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m<\u001B[0m \u001B[0mmin_valid_loss\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 90\u001B[1;33m         \u001B[0msave_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean_train_losses\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmean_valid_losses\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maccuracy\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     91\u001B[0m         \u001B[0mepochs_no_improve\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;36m0\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     92\u001B[0m         \u001B[0mmin_valid_loss\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mmean_valid_losses\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;33m-\u001B[0m\u001B[1;36m1\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m<ipython-input-80-fc747efe7be8>\u001B[0m in \u001B[0;36msave_model\u001B[1;34m(epoch, model, optimizer, train_loss, valid_loss, accuracy)\u001B[0m\n\u001B[0;32m      3\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0msave_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moptimizer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mvalid_loss\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0maccuracy\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      4\u001B[0m     \u001B[0mPATH\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m'current_state.pt'\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 5\u001B[1;33m     torch.save({\n\u001B[0m\u001B[0;32m      6\u001B[0m         \u001B[1;34m'epoch'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mepoch\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      7\u001B[0m         \u001B[1;34m'model_state_dict'\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstate_dict\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\skola_fiit\\neural_networks_at_fiit\\zadanie 2\\.env_zadanie2\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36msave\u001B[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001B[0m\n\u001B[0;32m    367\u001B[0m     \u001B[0m_check_dill_version\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mpickle_module\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 369\u001B[1;33m     \u001B[1;32mwith\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mf\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;34m'wb'\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mopened_file\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    370\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[0m_use_new_zipfile_serialization\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    371\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0m_open_zipfile_writer\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopened_file\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32mas\u001B[0m \u001B[0mopened_zipfile\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\skola_fiit\\neural_networks_at_fiit\\zadanie 2\\.env_zadanie2\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m_open_file_like\u001B[1;34m(name_or_buffer, mode)\u001B[0m\n\u001B[0;32m    228\u001B[0m \u001B[1;32mdef\u001B[0m \u001B[0m_open_file_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    229\u001B[0m     \u001B[1;32mif\u001B[0m \u001B[0m_is_path\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 230\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname_or_buffer\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    231\u001B[0m     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    232\u001B[0m         \u001B[1;32mif\u001B[0m \u001B[1;34m'w'\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\skola_fiit\\neural_networks_at_fiit\\zadanie 2\\.env_zadanie2\\lib\\site-packages\\torch\\serialization.py\u001B[0m in \u001B[0;36m__init__\u001B[1;34m(self, name, mode)\u001B[0m\n\u001B[0;32m    209\u001B[0m \u001B[1;32mclass\u001B[0m \u001B[0m_open_file\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_opener\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    210\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 211\u001B[1;33m         \u001B[0msuper\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_open_file\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__init__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mopen\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmode\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    212\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    213\u001B[0m     \u001B[1;32mdef\u001B[0m \u001B[0m__exit__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "run = wandb.init(project='Zadanie2-CIFAR', entity='xpetricko')\n",
    "run.name = f\"Multitask run 2-{epochs}e-{batch_size}bs-vgg11_bn\"\n",
    "\n",
    "\n",
    "config = wandb.config\n",
    "config.learning_rate = learning_rate\n",
    "config.sample_limit=sample_limit\n",
    "config.batch_size=batch_size\n",
    "config.epochs_with_no_improve =  6\n",
    "config.notes='update_architecture'\n",
    "\n",
    "\n",
    "model = NetMultitask(3,100,20)\n",
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(),lr=learning_rate, weight_decay = 0.1) # pri adamW weight_decay = 1e-2\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "X_train,X_valid,y_train,y_valid = train_test_split_v2(X_train_np,\n",
    "                                                      y_train_np,\n",
    "                                                      sample_limit=sample_limit,\n",
    "                                                      test_size=1/6,\n",
    "                                                      random_state=7)\n",
    "\n",
    "_,_,y_super_train,y_super_valid = train_test_split_v2(X_train_np,\n",
    "                                                      y_super_train_np,\n",
    "                                                      sample_limit=sample_limit,\n",
    "                                                      test_size=1/6,\n",
    "                                                      random_state=7)\n",
    "\n",
    "train_data = CIFAR_Multitask(X_train,y_train,y_super_train, transform=transforms.ToTensor())\n",
    "valid_data = CIFAR(X_valid,y_valid, transform=transforms.ToTensor())\n",
    "\n",
    "train_loader = DataLoader(dataset=train_data,batch_size=batch_size)\n",
    "valid_loader = DataLoader(dataset=valid_data,batch_size=batch_size)\n",
    "\n",
    "mean_train_losses = []\n",
    "mean_valid_losses = []\n",
    "valid_acc_list = []\n",
    "min_valid_loss = np.Inf\n",
    "\n",
    "\n",
    "torch.manual_seed(7)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    for i, (images,labels, super_labels) in enumerate(train_loader):\n",
    "        images, labels, super_labels = images.to(device), labels.to(device), super_labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output, super_output = model.forward(images)\n",
    "        loss1 = loss_fn(output,labels)\n",
    "        loss2 = loss_fn(super_output,super_labels)\n",
    "        loss= loss1+loss2\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i,(images,labels) in enumerate(valid_loader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output,_ = model.forward(images)\n",
    "            loss = loss_fn(output,labels)\n",
    "\n",
    "            valid_losses.append(loss.item())\n",
    "\n",
    "            _, predicted = torch.max(output.data,1)\n",
    "            correct += (predicted==labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    mean_train_losses.append(np.mean(train_losses))\n",
    "    mean_valid_losses.append(np.mean(valid_losses))\n",
    "    accuracy = 100*correct/total\n",
    "    valid_acc_list.append(accuracy)\n",
    "\n",
    "\n",
    "    wandb.log({\"Train mean loss\":mean_train_losses[-1],\"Valid mean loss\":mean_valid_losses[-1],\"Accuracy\":accuracy})\n",
    "\n",
    "    print('Epoch: {}, train loss: {:.4f}, valid loss: {:.4f}, valid acc: {:.2f}%'.format(epoch+1,mean_train_losses[-1],mean_valid_losses[-1],accuracy))\n",
    "\n",
    "    if(mean_valid_losses[-1] < min_valid_loss):\n",
    "        save_model(epoch, model, optimizer, mean_train_losses[-1], mean_valid_losses[-1], accuracy)\n",
    "        epochs_no_improve = 0\n",
    "        min_valid_loss = mean_valid_losses[-1]\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if epoch > 5 and epochs_no_improve == n_epochs_stop:\n",
    "        print(f'Early Stopping on epoch {epoch}/{epochs}, min valid loss: {min_valid_loss}, last valid loss: {mean_valid_losses[-1]}')\n",
    "#         early_stop = True\n",
    "        break\n",
    "torch.cuda.empty_cache()\n",
    "run.finish()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "netenv",
   "language": "python",
   "name": "netenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}